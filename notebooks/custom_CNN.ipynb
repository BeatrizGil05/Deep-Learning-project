{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DD2XbwqhCzp"
      },
      "source": [
        "## **Custom CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook documents the creation and training of the custom Convolutional Neural Network (CNN). This serves as the baseline model against which the performance of the transfer learning models will be compared.\n",
        "\n",
        "Goal: Train a Custom CNN using 80/10/10 stratified split, 0-1 normalization, data augmentation, and class weights (due to class imbalance)."
      ],
      "metadata": {
        "id": "wWkCZkeIhDui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Setup and Initialization"
      ],
      "metadata": {
        "id": "Yb4hZRw_iY26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and Paths"
      ],
      "metadata": {
        "id": "DuRMFAJhikBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import os"
      ],
      "metadata": {
        "id": "PhLfvP6fh7sY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import utility functions from  uploaded files\n",
        "from data_utils import perform_stratified_split, DataGeneratorUtils, TARGET_SIZE, SEED\n",
        "from train_utils import compile_model, get_callbacks"
      ],
      "metadata": {
        "id": "54VzNtcwh-0T",
        "outputId": "2b449b02-80b8-43be-b6f2-141d8063213f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data_utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1227416924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import utility functions from  uploaded files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mperform_stratified_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataGeneratorUtils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cmF8G_xIjDgk",
        "outputId": "ccb22053-d9a3-42f8-d9c8-0011fbf02cff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ixH-8SqhCzs"
      },
      "outputs": [],
      "source": [
        "# Path to your metadata.csv file\n",
        "METADATA_PATH = \"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/rare_species/metadata.csv\"\n",
        "# Root directory containing all your original, un-split image files\n",
        "IMAGE_ROOT_DIR = \"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/rare_species\"\n",
        "# The target folder where the stratified data structure (train/val/test) will be created\n",
        "DATA_TARGET_DIR = \"/content/rive/MyDrive/NOVA_IMS/Deep_Learning_Project/data\"\n",
        "\n",
        "# Create the results directory if it doesn't exist to save model weights\n",
        "os.makedirs(\"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting and Generator Creation\n",
        "\n",
        "*We use the functions from data_utils.py to handle the reproducible split and the data pipeline creation.*"
      ],
      "metadata": {
        "id": "VLCWiewJimxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the metadata file\n",
        "try:\n",
        "    metadata_df = pd.read_csv(METADATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Metadata file not found.\")"
      ],
      "metadata": {
        "id": "1W6HW4tBi1rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN perform_stratified_split ONLY ONCE!\n",
        "#data_base_path = perform_stratified_split(metadata_df, IMAGE_ROOT_DIR, DATA_TARGET_DIR)\n",
        "#print(f\"Data structure created/verified at: {data_base_path}\")"
      ],
      "metadata": {
        "id": "3DUjltjXjotW",
        "outputId": "c6d23958-5a75-4762-8a79-80e459baf3a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Split: Train=9585, Validation=1199, Test=1199\n",
            "Organizing train set...\n",
            "Organizing validation set...\n",
            "Organizing test set...\n",
            "Data directory structure successfully created/updated.\n",
            "Data structure created/verified at: /content/rive/MyDrive/NOVA_IMS/Deep_Learning_Project/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls -R /content/rive\n"
      ],
      "metadata": {
        "id": "UYm2R3xKvpT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: there is a typo in the path file - rive instead of drive"
      ],
      "metadata": {
        "id": "bDIbQuvhvh5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verification: counting images per class in split directories"
      ],
      "metadata": {
        "id": "eWLDX8dIwXv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_images_per_class(base_directory, set_name):\n",
        "    \"\"\"Counts the number of images in each class (family folder) for a given set.\"\"\"\n",
        "    directory = os.path.join(base_directory, set_name)\n",
        "    class_counts = {}\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        return {f\"ERROR: Directory not found at {directory}\": 0}\n",
        "\n",
        "    for folder in os.listdir(directory):\n",
        "        folder_path = os.path.join(directory, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Count files in the class folder\n",
        "            image_count = len([f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            class_counts[folder] = image_count\n",
        "    return class_counts"
      ],
      "metadata": {
        "id": "tuGGvraUwhEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count images in train, validation, and test directories\n",
        "train_class_counts = count_images_per_class(DATA_TARGET_DIR, 'train')\n",
        "val_class_counts = count_images_per_class(DATA_TARGET_DIR, 'validation')\n",
        "test_class_counts = count_images_per_class(DATA_TARGET_DIR, 'test')\n",
        "\n",
        "# Display results\n",
        "print(\"\\n--- Class Counts Verification ---\")\n",
        "print(\"Number of images per class in the TRAIN directory (Top 5):\")\n",
        "# Sort and print for readability\n",
        "print(pd.Series(train_class_counts).sort_values(ascending=False).head(5))\n",
        "\n",
        "print(\"\\nNumber of images per class in the VALIDATION directory (Top 5):\")\n",
        "print(pd.Series(val_class_counts).sort_values(ascending=False).head(5))\n",
        "\n",
        "print(\"\\nNumber of images per class in the TEST directory (Top 5):\")\n",
        "print(pd.Series(test_class_counts).sort_values(ascending=False).head(5))\n",
        "\n",
        "# Check for overall size consistency\n",
        "total_counted = sum(train_class_counts.values()) + sum(val_class_counts.values()) + sum(test_class_counts.values())\n",
        "print(f\"\\nTotal images successfully counted across all splits: {total_counted}\")"
      ],
      "metadata": {
        "id": "mtb37uqGvyp2",
        "outputId": "a52150d9-a53e-4235-a752-af984522f0d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Class Counts Verification ---\n",
            "Number of images per class in the TRAIN directory (Top 5):\n",
            "cercopithecidae    240\n",
            "dactyloidae        240\n",
            "formicidae         233\n",
            "salamandridae      216\n",
            "plethodontidae     216\n",
            "dtype: int64\n",
            "\n",
            "Number of images per class in the VALIDATION directory (Top 5):\n",
            "cercopithecidae    30\n",
            "dactyloidae        30\n",
            "formicidae         29\n",
            "salamandridae      27\n",
            "plethodontidae     27\n",
            "dtype: int64\n",
            "\n",
            "Number of images per class in the TEST directory (Top 5):\n",
            "cercopithecidae    30\n",
            "dactyloidae        30\n",
            "formicidae         29\n",
            "salamandridae      27\n",
            "plethodontidae     27\n",
            "dtype: int64\n",
            "\n",
            "Total images successfully counted across all splits: 11983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_base_path = DATA_TARGET_DIR\n"
      ],
      "metadata": {
        "id": "CZbMNijjyinZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Initialize Data Generators\n",
        "data_util = DataGeneratorUtils(data_base_path)\n",
        "\n",
        "train_generator = data_util.create_generators('train')\n",
        "val_generator = data_util.create_generators('validation')\n"
      ],
      "metadata": {
        "id": "lDivEMsJiUeo",
        "outputId": "4f979b8f-820e-4e5b-ccbe-2bef701eaf06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9585 images belonging to 202 classes.\n",
            "Found 1199 images belonging to 202 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = train_generator.num_classes"
      ],
      "metadata": {
        "id": "rlNpZvTDysq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Class Weights (Crucial for rare species imbalance)\n",
        "class_weights = data_util.calculate_class_weights(train_generator)\n",
        "\n",
        "print(f\"\\nSetup complete. Total classes: {NUM_CLASSES}\")\n",
        "print(f\"Train samples: {train_generator.samples}, Validation samples: {val_generator.samples}\")"
      ],
      "metadata": {
        "id": "a8349i6nyoyj",
        "outputId": "c91b7779-c70e-429a-ca99-d4ad841bf50b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights calculated for 202 classes.\n",
            "\n",
            "Setup complete. Total classes: 202\n",
            "Train samples: 9585, Validation samples: 1199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuner and Callbacks"
      ],
      "metadata": {
        "id": "rh5U2QTH9UmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "WAouPImv9NKN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = 'custom_cnn_hyperband'"
      ],
      "metadata": {
        "id": "04Tflw4g9fwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMUz4GW09x9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping: Stops training if val_loss doesn't improve for 5 epochs\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "80UNg-Mn9ivN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce LR on Plateau: Reduces LR if val_loss doesn't improve for 5 epochs\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "32wlCDr39oE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [early_stopping, reduce_lr]"
      ],
      "metadata": {
        "id": "poXLi0yu9qnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the Hyperband Tuner\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel=build_hypermodel,\n",
        "    objective='val_accuracy', # Maximize validation accuracy\n",
        "    max_epochs=30,           # Max epochs for a full training run\n",
        "    factor=3,                # Halving factor for Hyperband\n",
        "    directory='/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs', # Path to save results\n",
        "    project_name=PROJECT_NAME,\n",
        "    overwrite=True           # Overwrite previous search results\n",
        ")\n",
        "\n",
        "print(\"Tuner search space summary:\")\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "id": "KA8yN5pi91aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the search using the training and validation generators and class weights\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    epochs=30,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "FzpFTkqn96rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFmhAW-5-AVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZKM-ZKGhCzt"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}