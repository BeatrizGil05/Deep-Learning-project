{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DD2XbwqhCzp"
      },
      "source": [
        "## **Custom CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWkCZkeIhDui"
      },
      "source": [
        "This notebook documents the creation and training of the custom Convolutional Neural Network (CNN). This serves as the baseline model against which the performance of the transfer learning models will be compared.\n",
        "\n",
        "Goal: Train a Custom CNN using 80/10/10 stratified split, 0-1 normalization, data augmentation, and class weights (due to class imbalance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb4hZRw_iY26"
      },
      "source": [
        "## Project Setup and Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuRMFAJhikBQ"
      },
      "source": [
        "### Imports and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PhLfvP6fh7sY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "54VzNtcwh-0T"
      },
      "outputs": [],
      "source": [
        "# Import utility functions from  uploaded files\n",
        "from data_utils import perform_stratified_split, DataGeneratorUtils, TARGET_SIZE, SEED, BATCH_SIZE\n",
        "from train_utils import compile_model, get_callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmF8G_xIjDgk",
        "outputId": "8aaafa61-a151-4ebc-e073-6d7b82cdc5f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5ixH-8SqhCzs"
      },
      "outputs": [],
      "source": [
        "# Path to your metadata.csv file\n",
        "METADATA_PATH = \"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/rare_species/metadata.csv\"\n",
        "\n",
        "# Root directory containing all your original, un-split image files\n",
        "IMAGE_ROOT_DIR = \"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/rare_species\"\n",
        "\n",
        "# The target folder where the stratified data structure (train/val/test) will be created\n",
        "DATA_TARGET_DIR = \"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/data\"\n",
        "\n",
        "# Create the results directory if it doesn't exist to save model weights\n",
        "os.makedirs(\"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLCWiewJimxC"
      },
      "source": [
        "## Data Splitting and Generator Creation\n",
        "\n",
        "We use the functions from data_utils.py to handle the reproducible split and the data pipeline creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1W6HW4tBi1rZ"
      },
      "outputs": [],
      "source": [
        "# Load the metadata file\n",
        "try:\n",
        "    metadata_df = pd.read_csv(METADATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Metadata file not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DUjltjXjotW",
        "outputId": "b9d204ab-3b0d-4d33-d9f7-28d5dadb3a02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Split: Train=9585, Validation=1199, Test=1199\n",
            "Organizing train set...\n",
            "Organizing validation set...\n",
            "Organizing test set...\n",
            "Data directory structure successfully created/updated.\n",
            "Data structure created/verified at: /content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/data\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN perform_stratified_split ONLY ONCE!\n",
        "#data_base_path = perform_stratified_split(metadata_df, IMAGE_ROOT_DIR, DATA_TARGET_DIR)\n",
        "#print(f\"Data structure created/verified at: {data_base_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UYm2R3xKvpT-"
      },
      "outputs": [],
      "source": [
        "#!ls -R /content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWLDX8dIwXv-"
      },
      "source": [
        "### Verification: counting images per class in split directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tuGGvraUwhEI"
      },
      "outputs": [],
      "source": [
        "def count_images_per_class(base_directory, set_name):\n",
        "    \"\"\"Counts the number of images in each class (family folder) for a given set.\"\"\"\n",
        "    directory = os.path.join(base_directory, set_name)\n",
        "    class_counts = {}\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        return {f\"ERROR: Directory not found at {directory}\": 0}\n",
        "\n",
        "    for folder in os.listdir(directory):\n",
        "        folder_path = os.path.join(directory, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Count files in the class folder\n",
        "            image_count = len([f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            class_counts[folder] = image_count\n",
        "    return class_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW8ztJNDJI7z",
        "outputId": "c34feac8-c69a-4f29-90a5-332ce7c21e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Class Counts Verification ---\n",
            "Number of images per class in the TRAIN directory (Top 5):\n",
            "cercopithecidae    240\n",
            "dactyloidae        240\n",
            "formicidae         233\n",
            "plethodontidae     216\n",
            "carcharhinidae     216\n",
            "dtype: int64\n",
            "\n",
            "Number of images per class in the VALIDATION directory (Top 5):\n",
            "cercopithecidae    30\n",
            "dactyloidae        30\n",
            "formicidae         29\n",
            "plethodontidae     27\n",
            "carcharhinidae     27\n",
            "dtype: int64\n",
            "\n",
            "Number of images per class in the TEST directory (Top 5):\n",
            "dactyloidae        30\n",
            "cercopithecidae    30\n",
            "formicidae         29\n",
            "plethodontidae     27\n",
            "salamandridae      27\n",
            "dtype: int64\n",
            "\n",
            "Total images successfully counted across all splits: 11983\n"
          ]
        }
      ],
      "source": [
        "# Count images in train, validation, and test directories\n",
        "train_class_counts = count_images_per_class(DATA_TARGET_DIR, 'train')\n",
        "val_class_counts = count_images_per_class(DATA_TARGET_DIR, 'validation')\n",
        "test_class_counts = count_images_per_class(DATA_TARGET_DIR, 'test')\n",
        "\n",
        "# Display results\n",
        "print(\"\\n--- Class Counts Verification ---\")\n",
        "print(\"Number of images per class in the TRAIN directory (Top 5):\")\n",
        "# Sort and print for readability\n",
        "print(pd.Series(train_class_counts).sort_values(ascending=False).head(5))\n",
        "\n",
        "print(\"\\nNumber of images per class in the VALIDATION directory (Top 5):\")\n",
        "print(pd.Series(val_class_counts).sort_values(ascending=False).head(5))\n",
        "\n",
        "print(\"\\nNumber of images per class in the TEST directory (Top 5):\")\n",
        "print(pd.Series(test_class_counts).sort_values(ascending=False).head(5))\n",
        "\n",
        "# Check for overall size consistency\n",
        "total_counted = sum(train_class_counts.values()) + sum(val_class_counts.values()) + sum(test_class_counts.values())\n",
        "print(f\"\\nTotal images successfully counted across all splits: {total_counted}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CZbMNijjyinZ"
      },
      "outputs": [],
      "source": [
        "data_base_path = DATA_TARGET_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDivEMsJiUeo",
        "outputId": "5d1fde4e-6041-49cd-8b65-a716bdaae31c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9585 images belonging to 202 classes.\n",
            "Found 1199 images belonging to 202 classes.\n"
          ]
        }
      ],
      "source": [
        "#  Initialize Data Generators\n",
        "data_util = DataGeneratorUtils(data_base_path)\n",
        "\n",
        "train_generator = data_util.create_generators('train')\n",
        "val_generator = data_util.create_generators('validation')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rlNpZvTDysq9"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = train_generator.num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8349i6nyoyj",
        "outputId": "22be4766-5549-4aa5-aa48-696b6ab26736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights calculated for 202 classes.\n",
            "\n",
            "Setup complete. Total classes: 202\n",
            "Train samples: 9585, Validation samples: 1199\n"
          ]
        }
      ],
      "source": [
        "# Calculate Class Weights (Crucial for rare species imbalance)\n",
        "class_weights = data_util.calculate_class_weights(train_generator)\n",
        "\n",
        "print(f\"\\nSetup complete. Total classes: {NUM_CLASSES}\")\n",
        "print(f\"Train samples: {train_generator.samples}, Validation samples: {val_generator.samples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Transfer to Local Disk for Fast I/O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cells zip the data on Drive and unzip it locally to speed up the tuning and training process significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lue-HXWZOX3",
        "outputId": "1514517a-bc3b-46a5-d292-1f78453e8f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating archive from: /content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/data\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n",
            "\n",
            "--- Zipping Complete ---\n",
            "Archive created at: /content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/data.zip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the location of the directory you want to archive (the source)\n",
        "SOURCE_DATA_DIR = \"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/data\"\n",
        "\n",
        "# Define the desired path and name for the output ZIP file\n",
        "OUTPUT_ZIP_PATH = \"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/data.zip\"\n",
        "\n",
        "print(f\"Creating archive from: {SOURCE_DATA_DIR}\")\n",
        "\n",
        "# Navigate to the parent directory of 'data' (which is 'Deep_Learning_Project')\n",
        "os.chdir(os.path.dirname(SOURCE_DATA_DIR))\n",
        "\n",
        "# Execute the zip command\n",
        "!zip -r -q {OUTPUT_ZIP_PATH} data/\n",
        "\n",
        "print(\"\\n--- Zipping Complete ---\")\n",
        "print(f\"Archive created at: {OUTPUT_ZIP_PATH}\")\n",
        "\n",
        "# Navigate back to a safe location\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2St9o3PgbRtG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5GTzxj6xqeeS"
      },
      "outputs": [],
      "source": [
        "# Navigate back to a safe location\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CD2aQuTHbPUq"
      },
      "outputs": [],
      "source": [
        "# Source ZIP file on Google Drive \n",
        "GDRIVE_ZIP_PATH = \"/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/data.zip\"\n",
        "\n",
        "# Destination folder for the unzipped data on the local Colab disk\n",
        "LOCAL_DATA_DIR = \"/content/data\"\n",
        "LOCAL_ZIP_PATH = \"/content/data.zip\"\n",
        "\n",
        "# Create the local directory\n",
        "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJDS59LLb5yo",
        "outputId": "150a1dbd-f422-4779-ab67-1d110fef0f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying data from Google Drive to local disk...\n",
            "Unzipping data...\n",
            "\n",
            "Data transfer complete\n"
          ]
        }
      ],
      "source": [
        "#  Copy and Unzip\n",
        "print(\"Copying data from Google Drive to local disk...\")\n",
        "# Copy the single zip file (this is fast)\n",
        "shutil.copy(GDRIVE_ZIP_PATH, LOCAL_ZIP_PATH)\n",
        "\n",
        "# Unzip the file onto the local disk\n",
        "print(\"Unzipping data...\")\n",
        "# -q makes it quiet, -d sets the destination folder\n",
        "!unzip -q {LOCAL_ZIP_PATH} -d {LOCAL_DATA_DIR}\n",
        "\n",
        "print(\"\\nData transfer complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc9Jnbv-bMjP",
        "outputId": "2fabfdfa-3afa-4cfe-804c-18fe24668c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Re-initializing generators using the faster local path: /content/data/data\n",
            "Found 9585 images belonging to 202 classes.\n",
            "Found 1199 images belonging to 202 classes.\n",
            "Class weights calculated for 202 classes.\n",
            "\n",
            "Generators are ready. Total classes: 202\n"
          ]
        }
      ],
      "source": [
        "# Re-initialize Generators with Local Path\n",
        "\n",
        "# Point the base path to the directory containing 'train', 'validation', and 'test'.\n",
        "FAST_I_O_BASE_PATH = os.path.join(LOCAL_DATA_DIR, 'data')\n",
        "\n",
        "print(f\"\\nRe-initializing generators using the faster local path: {FAST_I_O_BASE_PATH}\")\n",
        "\n",
        "# Re-initialize Data Generators with the fast, local path\n",
        "data_util = DataGeneratorUtils(FAST_I_O_BASE_PATH)\n",
        "\n",
        "train_generator = data_util.create_generators('train')\n",
        "val_generator = data_util.create_generators('validation')\n",
        "\n",
        "# Recalculate class weights (important to run after generator initialization)\n",
        "class_weights = data_util.calculate_class_weights(train_generator)\n",
        "\n",
        "NUM_CLASSES = train_generator.num_classes\n",
        "print(f\"\\nGenerators are ready. Total classes: {NUM_CLASSES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh5U2QTH9UmP"
      },
      "source": [
        "# Hyperparameter Tuning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WAouPImv9NKN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBoCBXXwrKcI",
        "outputId": "c772abed-485a-460f-8e25-babe484a8dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.8-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (2.32.4)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (1.76.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (5.29.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio->keras_tuner) (4.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras_tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras_tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.8-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.8 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I5sbEnzVWGAt"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "m3oWoQa-dl94"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "04Tflw4g9fwk"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = 'custom_cnn_hyperband'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defininig Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "80UNg-Mn9ivN"
      },
      "outputs": [],
      "source": [
        "# Early Stopping: Stops training if val_loss doesn't improve for 5 epochs\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "32wlCDr39oE_"
      },
      "outputs": [],
      "source": [
        "# Reduce LR on Plateau: Reduces LR if val_loss doesn't improve for 5 epochs\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "poXLi0yu9qnc"
      },
      "outputs": [],
      "source": [
        "callbacks = [early_stopping, reduce_lr]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Hypermodel Definition\n",
        "This model defines a fixed two-block CNN architecture and focuses on tuning the most impactful parameters: the initial filter size, dropout rate, and learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ik46Xy-AWYwN"
      },
      "outputs": [],
      "source": [
        "def build_hypermodel(hp):\n",
        "    \"\"\"\n",
        "    Builds a Keras Sequential model parameterized by KerasTuner's HyperParameters (hp).\n",
        "    Input shape is now (128, 128, 3).\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Tunable Hyperparameters\n",
        "    hp_filters_1 = hp.Int('filters_1', min_value=32, max_value=128, step=32)\n",
        "    hp_dropout = hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 5e-5])\n",
        "\n",
        "    # Model Architecture\n",
        "    model.add(Conv2D(hp_filters_1, (3, 3), activation='relu',\n",
        "                     input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Classification Head\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(hp_dropout))\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "    # Compile Model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA8yN5pi91aR",
        "outputId": "22c582f4-4a14-42aa-e9aa-b786d1859e89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuner search space summary:\n",
            "Search space summary\n",
            "Default search space size: 3\n",
            "filters_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "dropout (Float)\n",
            "{'default': 0.2, 'conditions': [], 'min_value': 0.2, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001, 5e-05], 'ordered': True}\n"
          ]
        }
      ],
      "source": [
        "#Instantiate the Hyperband Tuner\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel=build_hypermodel,\n",
        "    objective='val_accuracy', # Maximize validation accuracy\n",
        "    max_epochs=30,           # Max epochs for a full training run\n",
        "    factor=3,                # Halving factor for Hyperband\n",
        "    directory='/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs', # Path to save results\n",
        "    project_name=PROJECT_NAME,\n",
        "    overwrite=True           # Overwrite previous search results\n",
        ")\n",
        "\n",
        "print(\"Tuner search space summary:\")\n",
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "FzpFTkqn96rU",
        "outputId": "4c4a5515-c383-4609-dee6-76c7622c0654"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tuner' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Run the search using the training and validation generators and class weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tuner\u001b[39m.\u001b[39msearch(\n\u001b[1;32m      3\u001b[0m     train_generator,\n\u001b[1;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m      5\u001b[0m     validation_data\u001b[39m=\u001b[39mval_generator,\n\u001b[1;32m      6\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m      7\u001b[0m     class_weight\u001b[39m=\u001b[39mclass_weights,\n\u001b[1;32m      8\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
          ]
        }
      ],
      "source": [
        "# Run the search using the training and validation generators and class weights\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C1V2Qd3oU_q"
      },
      "source": [
        "The trial was not completed due to limited computational resources, and thus, no definitive or optimal hyperparameters were saved from this initial search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subsampling and Redefining a New Generator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the full training set is large, we use a stratified 20% subset of the training data for faster hyperparameter searching. We use the full validation set for reliable evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FLzwmcjeon61"
      },
      "outputs": [],
      "source": [
        "# Path to the training directory on the fast local disk\n",
        "TRAIN_ROOT = \"/content/data/data/train\"\n",
        "SUBSET_RATIO = 0.20 # Use 20% of the data for tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NcKPzHhTosCP"
      },
      "outputs": [],
      "source": [
        "# Gather all file paths and labels\n",
        "file_paths = []\n",
        "labels = []\n",
        "for family_class in os.listdir(TRAIN_ROOT):\n",
        "    family_path = os.path.join(TRAIN_ROOT, family_class)\n",
        "    if os.path.isdir(family_path):\n",
        "        for img_file in os.listdir(family_path):\n",
        "            # We save a relative path that flow_from_dataframe can use\n",
        "            file_paths.append(os.path.join(family_class, img_file))\n",
        "            labels.append(family_class)\n",
        "\n",
        "full_train_df = pd.DataFrame({'filename': file_paths, 'class': labels})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JJM8HVeJoxCw"
      },
      "outputs": [],
      "source": [
        "#  Perform Stratified Split for the Subset\n",
        "# Stratified sampling ensures all 202 classes are present in the 20% subset.\n",
        "_, subset_df = train_test_split(\n",
        "    full_train_df,\n",
        "    test_size=SUBSET_RATIO,\n",
        "    random_state=SEED,\n",
        "    stratify=full_train_df['class']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Gywo__57o_YE"
      },
      "outputs": [],
      "source": [
        "# Define the subset generator\n",
        "subset_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, # 0-1 Normalization\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyPtHJpVpNHt",
        "outputId": "5426a77c-81c6-4126-b854-58d5eab05db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1917 validated image filenames belonging to 202 classes.\n"
          ]
        }
      ],
      "source": [
        "# Create the final subset generator\n",
        "subset_generator = subset_datagen.flow_from_dataframe(\n",
        "    subset_df,\n",
        "    directory=TRAIN_ROOT, # The absolute root where the image folders are located\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFmhAW-5-AVA",
        "outputId": "7a6c84ed-3f73-4b9b-80f5-eacd219f2480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original training samples: 9585, Tuning subset samples: 1917\n",
            "New steps per epoch for tuning: 30.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Original training samples: {len(full_train_df)}, Tuning subset samples: {len(subset_df)}\")\n",
        "print(f\"New steps per epoch for tuning: {np.ceil(len(subset_df) / BATCH_SIZE)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qtnPuBoHsAmh"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = 'custom_cnn_hyperband_fastest'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Hyperparameter Tuner on a Sub-Sampled Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLvK97iFsDcB",
        "outputId": "06bdf8b1-9098-44f6-805a-ecd206d7a9b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "tuner = kt.Hyperband(\n",
        "    hypermodel=build_hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=15,           # Budget sufficient for proxy tuning\n",
        "    factor=3,\n",
        "    directory='/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs',\n",
        "    project_name=PROJECT_NAME,\n",
        "    overwrite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPoTncIdsLEc",
        "outputId": "307f45c9-4d36-40d6-8b34-33ba420f75d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 07m 51s]\n",
            "val_accuracy: 0.01668056659400463\n",
            "\n",
            "Best val_accuracy So Far: 0.01668056659400463\n",
            "Total elapsed time: 02h 44m 48s\n"
          ]
        }
      ],
      "source": [
        "tuner.search(\n",
        "    subset_generator,\n",
        "    epochs=15,\n",
        "    validation_data=val_generator, # Use the full val_generator\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiuT1sD6UPXS",
        "outputId": "140a2378-a2d8-48d8-bef7-c4ef6549e8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal Learning Rate: 0.00005\n"
          ]
        }
      ],
      "source": [
        "# Get the best hyperparameters found\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Optimal Learning Rate: {best_hps.get('learning_rate'):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q2BajgNVCo0",
        "outputId": "40f8e09f-0c07-4166-f94d-3976169ca621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal Filters (Block 1): 128\n",
            "Optimal Dropout Rate: 0.20\n"
          ]
        }
      ],
      "source": [
        "print(f\"Optimal Filters (Block 1): {best_hps.get('filters_1')}\")\n",
        "print(f\"Optimal Dropout Rate: {best_hps.get('dropout'):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "leQvjJg-UK3B"
      },
      "outputs": [],
      "source": [
        "# Get the best model and save weights\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "FINAL_MODEL_NAME_TUNING = 'custom_cnn_failed_tuning'\n",
        "best_model_filepath = f'/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs/best_model_weights_{FINAL_MODEL_NAME_TUNING}.weights.h5'\n",
        "best_model.save_weights(best_model_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary\n",
        "* Status: Search completed 30 trials over approximately 2 hours and 44 minutes.\n",
        "\n",
        "* Best Performance: The highest validation accuracy achieved was only ≈1.67% (0.01668).\n",
        "\n",
        "* Conclusion: This performance is only slightly better than random guessing for a 202-class problem (random chance being ≈0.5%). The hyperparameters found were considered not optimal and not informative for building a functional final model.\n",
        "\n",
        "* Result: Due to the extremely poor performance of the best trial, the entire search was considered a failure to produce a viable baseline model. This necessitated the development of the Dynamic Hypermodel and the shift to the faster subset tuning strategy to find a more promising architecture and parameter set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Hypermodel Definition\n",
        "\n",
        "This model allows for tuning the number of convolutional blocks, filter size scaling, and dense layer size, in addition to dropout and learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZBs5rhbXlJ2",
        "outputId": "72e2fc42-4f89-44de-e918-a41c674d7993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hypermodel build function ready for DYNAMIC DEPTH tuning.\n"
          ]
        }
      ],
      "source": [
        "def build_hypermodel_deep(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    #  Tunable Hyperparameters\n",
        "\n",
        "    # 1. Learning Rate (Expanded Log Scale)\n",
        "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2,\n",
        "                                default=1e-3, sampling='log')\n",
        "\n",
        "    # 2. Number of Blocks (Tune Depth)\n",
        "    hp_num_blocks = hp.Int('num_blocks', min_value=3, max_value=5, step=1)\n",
        "\n",
        "    # 3. Starting Filters (Controls scaling of all blocks)\n",
        "    hp_filters_start = hp.Choice('filters_start', values=[32, 64])\n",
        "\n",
        "    # 4. Dropout rate\n",
        "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
        "\n",
        "    # 5. Dense layer size\n",
        "    hp_dense_units = hp.Int('dense_units', min_value=128, max_value=512, step=128)\n",
        "\n",
        "    #  MODEL ARCHITECTURE (DYNAMIC DEPTH)\n",
        "    input_shape = (TARGET_SIZE[0], TARGET_SIZE[1], 3)\n",
        "\n",
        "    # Loop to dynamically build the Conv/Pool Blocks\n",
        "    for i in range(hp_num_blocks):\n",
        "        # Double the filters in each successive block (e.g., 32, 64, 128, 256, 512...)\n",
        "        current_filters = hp_filters_start * (2 ** i)\n",
        "\n",
        "        # Add Conv Layer\n",
        "        model.add(Conv2D(current_filters, (3, 3), activation='relu',\n",
        "                         input_shape=input_shape if i == 0 else None))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Classification Head\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(hp_dropout))\n",
        "    model.add(Dense(hp_dense_units, activation='relu'))\n",
        "    model.add(Dropout(hp_dropout))\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "    # Compile Model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"Hypermodel build function ready for DYNAMIC DEPTH tuning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eNtYQG1xXwix"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = 'custom_cnn_ultimate_tune'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "3W9BnCefX8K2"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Hyperband Tuner\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel=build_hypermodel_deep,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory='/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs',\n",
        "    project_name=PROJECT_NAME,\n",
        "    overwrite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "OyWxDolmYB10",
        "outputId": "bf8deaba-a856-42d2-8bba-ad305fd0ee47"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tuner' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2050052566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the fast subset_generator for training and the full val_generator for  evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tuner.search(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msubset_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
          ]
        }
      ],
      "source": [
        "# Use the fast subset_generator for training and the full val_generator for evaluation\n",
        "tuner.search(\n",
        "    subset_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_3oj8cJRee2u"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 202"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ4EY8WHdYxm",
        "outputId": "ef9b1a29-e5a1-4ef1-9eef-98849b755e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuner object reloaded from Drive project: custom_cnn_ultimate_tune\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the Hyperband Tuner again with the exact same parameters\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel=build_hypermodel_deep,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory=OUTPUT_DIR, # The directory where the trials were saved\n",
        "    project_name=PROJECT_NAME,\n",
        "    overwrite=False # CRITICAL: Set to False to load previous results\n",
        ")\n",
        "\n",
        "print(f\"Tuner object reloaded from Drive project: {PROJECT_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkDGBPrufcZs",
        "outputId": "1e9b078c-0ae2-4162-e5cf-746ec0e731ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching /content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs/custom_cnn_ultimate_tune for completed trials...\n",
            "\n",
            "ERROR: Could not find any officially 'COMPLETED' trials. The search likely stopped too early.\n",
            "Please relaunch the tuner search to complete the remaining trials.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "PROJECT_NAME = 'custom_cnn_ultimate_tune'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/NOVA_IMS/Deep_Learning_Project/outputs'\n",
        "TRIAL_ROOT = os.path.join(OUTPUT_DIR, PROJECT_NAME)\n",
        "\n",
        "best_score = -1.0\n",
        "best_hps = None\n",
        "best_trial_id = None\n",
        "\n",
        "print(f\"Searching {TRIAL_ROOT} for completed trials...\")\n",
        "\n",
        "# Iterate through all folders in the project directory\n",
        "for trial_id in os.listdir(TRIAL_ROOT):\n",
        "    trial_path = os.path.join(TRIAL_ROOT, trial_id)\n",
        "\n",
        "    # Ensure it's a trial folder\n",
        "    if os.path.isdir(trial_path) and trial_id.startswith('trial_'):\n",
        "        trial_file = os.path.join(trial_path, 'trial.json')\n",
        "\n",
        "        if os.path.exists(trial_file):\n",
        "            try:\n",
        "                with open(trial_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                # We only consider trials that officially completed\n",
        "                if data['status'] == 'COMPLETED':\n",
        "\n",
        "                    # Extract the final score (max_value of val_accuracy)\n",
        "                    final_metrics = data.get('metrics', {}).get('metrics', {})\n",
        "                    score = final_metrics.get('val_accuracy', {}).get('max_value')\n",
        "\n",
        "                    if score is not None and score > best_score:\n",
        "                        best_score = score\n",
        "                        best_hps = data.get('hyperparameters', {}).get('values', {})\n",
        "                        best_trial_id = trial_id\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                # Ignore corrupted JSON files\n",
        "                continue\n",
        "\n",
        "if best_score > 0:\n",
        "    print(\"\\n--- ✅ MANUAL EXTRACTION SUCCESSFUL ---\")\n",
        "    print(f\"Best Validation Accuracy Found: {best_score:.5f}\")\n",
        "    print(f\"Trial ID: {best_trial_id}\")\n",
        "\n",
        "    print(\"\\n--- Optimal Hyperparameters Found ---\")\n",
        "\n",
        "    # Print the specific tunable parameters\n",
        "    print(f\"Optimal Learning Rate: {best_hps.get('learning_rate', 'N/A'):.5f}\")\n",
        "    print(f\"Optimal Number of Blocks: {best_hps.get('num_blocks', 'N/A')}\")\n",
        "    print(f\"Optimal Filters Start: {best_hps.get('filters_start', 'N/A')}\")\n",
        "    print(f\"Optimal Dropout Rate: {best_hps.get('dropout', 'N/A'):.2f}\")\n",
        "    print(f\"Optimal Dense Units: {best_hps.get('dense_units', 'N/A')}\")\n",
        "else:\n",
        "    print(\"\\nERROR: Could not find any officially 'COMPLETED' trials. The search likely stopped too early.\")\n",
        "    print(\"Please relaunch the tuner search to complete the remaining trials.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary\n",
        "Our second attempt at hyperparameter tuning, which involved exploring a larger set of parameters, was unsuccessful due to limited computational resources. Moreover, during the conducted trials, no significant performance improvement was observed, and no optimal parameter configuration could be identified. Consequently, we decided not to build and train a full CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZKM-ZKGhCzt"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (ai_course_env)",
      "language": "python",
      "name": "ai_course_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
