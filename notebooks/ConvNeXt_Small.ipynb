{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e193775a",
      "metadata": {
        "id": "e193775a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 22:33:58.759312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Complete Training Pipeline for Rare Species Classification\n",
        "Using ConvNeXt-Small with 3-stage fine-tuning strategy\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n",
        "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.config.optimizer.set_jit(False)  \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ConvNeXtSmall\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "99afab38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99afab38",
        "outputId": "0a93d968-fe05-4987-947d-a665a4e5b2ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GPU CHECK\n",
            "================================================================================\n",
            "✓ 1 GPU(s) available\n",
            "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Check GPU\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GPU CHECK\")\n",
        "print(\"=\"*80)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"✓ {len(gpus)} GPU(s) available\")\n",
        "    for gpu in gpus:\n",
        "        print(f\"  - {gpu}\")\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"⚠ No GPU detected - training will be slow!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b30a5340",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b30a5340",
        "outputId": "b33c0cc2-b842-413b-ae02-958418e8d7a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DATA PREPROCESSING\n",
            "================================================================================\n",
            "Number of missing files: 123\n",
            "Cleaned data: 11860 images\n",
            "Number of categories (families): 202\n",
            "Train: 9488\n",
            "Val: 1186\n",
            "Test: 1186\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df = pd.read_csv('../metadata.csv')\n",
        "data_root_path = \"/home/user/datasets/rare_species/\"\n",
        "df['full_path'] = df['file_path'].apply(lambda x: os.path.join(data_root_path, x))\n",
        "df = df.dropna(subset=['file_path', 'family']).reset_index(drop=True)\n",
        "df['exists'] = df['full_path'].apply(os.path.exists)\n",
        "print(f\"Number of missing files: {len(df[df['exists'] == False])}\")\n",
        "df = df[df['exists'] == True].reset_index(drop=True)\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "df = df.drop_duplicates(subset='full_path').reset_index(drop=True)\n",
        "print(f\"Cleaned data: {len(df)} images\")\n",
        "print(f\"Number of categories (families): {df['family'].nunique()}\")\n",
        "df['family'] = df['family'].astype(str)\n",
        "\n",
        "# Stratified split: 80% train, 10% val, 10% test\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.10,\n",
        "    stratify=df[\"family\"],\n",
        "    random_state=SEED\n",
        ")\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.1111,  # 0.1111 * 0.9 = 0.10\n",
        "    stratify=train_val_df[\"family\"],\n",
        "    random_state=SEED\n",
        ")\n",
        "print(f\"Train: {len(train_df)}\")\n",
        "print(f\"Val: {len(val_df)}\")\n",
        "print(f\"Test: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ca7f0d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ca7f0d0",
        "outputId": "2500fe7d-f8f0-4122-eb9e-14d04f3d821a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "BUILDING DATA GENERATORS\n",
            "================================================================================\n",
            "Found 9488 validated image filenames belonging to 202 classes.\n",
            "Found 1186 validated image filenames belonging to 202 classes.\n",
            "Found 1186 validated image filenames belonging to 202 classes.\n",
            "Train batches: 593\n",
            "Val batches: 75\n",
            "Test batches: 75\n",
            "Number of classes: 202\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BUILDING DATA GENERATORS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\"\"\"# ConvNeXt preprocessing\n",
        "def preprocess_convnext(x):\n",
        "    Normalize to [0, 1] for ConvNeXt\n",
        "    return x / 255.0\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_convnext,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.25,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.15,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_convnext\n",
        ")\"\"\"\n",
        "\n",
        "# balazs: Modified to use built-in ConvNeXt preprocessing because it wasnt learning at all...\n",
        "from tensorflow.keras.applications.convnext import preprocess_input as convnext_preprocess\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=convnext_preprocess,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.25,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.15,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=convnext_preprocess\n",
        ")\n",
        "\n",
        "train_ds = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='full_path',\n",
        "    y_col='family',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_ds = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='full_path',\n",
        "    y_col='family',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_ds = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='full_path',\n",
        "    y_col='family',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_ds)}\")\n",
        "print(f\"Val batches: {len(val_ds)}\")\n",
        "print(f\"Test batches: {len(test_ds)}\")\n",
        "print(f\"Number of classes: {len(train_ds.class_indices)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e1679664",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1679664",
        "outputId": "1e591e15-c1f6-4142-e51e-b78970f416e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "COMPUTING CLASS WEIGHTS\n",
            "================================================================================\n",
            "Class weights computed for 202 classes\n",
            "Weight range: 0.197 to 2.135\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPUTING CLASS WEIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "labels = train_ds.classes\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "class_weights = dict(enumerate(weights))\n",
        "print(f\"Class weights computed for {len(class_weights)} classes\")\n",
        "print(f\"Weight range: {min(weights):.3f} to {max(weights):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5f4cce94",
      "metadata": {
        "id": "5f4cce94"
      },
      "outputs": [],
      "source": [
        "def build_model(num_classes, trainable_backbone=False):\n",
        "    \"\"\"\n",
        "    Create ConvNeXt-Small model with custom head\n",
        "\n",
        "    Args:\n",
        "        num_classes: Number of output classes\n",
        "        trainable_backbone: Whether to make backbone trainable\n",
        "    \"\"\"\n",
        "    print(f\"\\nBuilding model (backbone trainable: {trainable_backbone})\")\n",
        "\n",
        "    # Load pretrained ConvNeXt-Small\n",
        "    base_model = ConvNeXtSmall(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
        "        pooling=None\n",
        "    )\n",
        "\n",
        "    base_model.trainable = trainable_backbone\n",
        "\n",
        "    # Build custom head\n",
        "    inputs = keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    print(f\"Total parameters: {model.count_params():,}\")\n",
        "    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def unfreeze_top_layers(model, percentage=0.3):\n",
        "    \"\"\"\n",
        "    Unfreeze top percentage of backbone layers\n",
        "\n",
        "    Args:\n",
        "        model: Keras model\n",
        "        percentage: Percentage of layers to unfreeze (from the end)\n",
        "    \"\"\"\n",
        "    base_model = model.layers[1]  # The ConvNeXt backbone\n",
        "    total_layers = len(base_model.layers)\n",
        "    unfreeze_from = int(total_layers * (1 - percentage))\n",
        "\n",
        "    print(f\"\\nUnfreezing top {percentage*100}% of backbone layers\")\n",
        "    print(f\"Total backbone layers: {total_layers}\")\n",
        "    print(f\"Unfreezing from layer {unfreeze_from} onwards\")\n",
        "\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:unfreeze_from]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
        "    print(f\"Trainable parameters after unfreezing: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dee0b78f",
      "metadata": {
        "id": "dee0b78f"
      },
      "outputs": [],
      "source": [
        "def get_callbacks(stage_name, patience=4):\n",
        "    \"\"\"Get training callbacks for a specific stage\"\"\"\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    callbacks = [\n",
        "        # Save best model\n",
        "        ModelCheckpoint(\n",
        "            filepath=f'model_{stage_name}_best.keras',\n",
        "            monitor='val_accuracy',\n",
        "            mode='max',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # Reduce learning rate when stuck\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1,\n",
        "            mode='max'\n",
        "        ),\n",
        "\n",
        "        # Early stopping\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=patience,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1,\n",
        "            mode='max'\n",
        "        ),\n",
        "\n",
        "        # TensorBoard logging\n",
        "        TensorBoard(\n",
        "            log_dir=f'logs/{stage_name}_{timestamp}',\n",
        "            histogram_freq=0\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "HyOK9bzXYK03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "HyOK9bzXYK03",
        "outputId": "22fa2372-7bf1-42fc-af1e-695fdce62029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "MODEL ARCHITECTURE\n",
            "================================================================================\n",
            "\n",
            "Building model (backbone trainable: False)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1765751652.357466   11747 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 49,954,090\n",
            "Trainable parameters: 498,378\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ convnext_small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">49,454,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">202</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">103,626</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ convnext_small (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │    \u001b[38;5;34m49,454,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m393,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m202\u001b[0m)            │       \u001b[38;5;34m103,626\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,954,090</span> (190.56 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m49,954,090\u001b[0m (190.56 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">498,378</span> (1.90 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m498,378\u001b[0m (1.90 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,455,712</span> (188.66 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m49,455,712\u001b[0m (188.66 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "    # Build model\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL ARCHITECTURE\")\n",
        "    print(\"=\"*80)\n",
        "    num_classes = len(train_ds.class_indices)\n",
        "    model = build_model(num_classes=num_classes, trainable_backbone=False)\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a20f6c2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a20f6c2f",
        "outputId": "36cbb3ee-3a7c-4e57-f247-804eebb2d0fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 1: Training with FROZEN backbone\n",
            "================================================================================\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-10 14:42:27.548021: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f0d5c0021c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2025-12-10 14:42:27.548084: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
            "2025-12-10 14:42:28.778701: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2025-12-10 14:42:33.700810: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91700\n",
            "2025-12-10 14:42:39.333571: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 14:42:41.442056: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 664 bytes spill stores, 664 bytes spill loads\n",
            "\n",
            "2025-12-10 14:42:42.062074: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 14:42:42.082041: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 14:42:42.159228: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-12-10 14:42:42.218212: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 14:42:43.233767: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 360 bytes spill stores, 360 bytes spill loads\n",
            "\n",
            "2025-12-10 14:42:46.844459: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=1,k4=3,k5=1,k6=0,k7=0,k19=0} for conv (f32[16,96,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,3,224,224]{3,2,1,0}, f32[96,3,4,4]{3,2,1,0}, f32[96]{0}), window={size=4x4 stride=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
            "2025-12-10 14:42:47.496363: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.65145943s\n",
            "Trying algorithm eng20{k2=1,k4=3,k5=1,k6=0,k7=0,k19=0} for conv (f32[16,96,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,3,224,224]{3,2,1,0}, f32[96,3,4,4]{3,2,1,0}, f32[96]{0}), window={size=4x4 stride=4x4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
            "2025-12-10 14:42:50.584194: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng3{k11=0} for conv (f32[16,192,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,192,28,28]{3,2,1,0}, f32[192,1,7,7]{3,2,1,0}), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, feature_group_count=192, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
            "2025-12-10 14:42:50.663882: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.079801896s\n",
            "Trying algorithm eng3{k11=0} for conv (f32[16,192,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,192,28,28]{3,2,1,0}, f32[192,1,7,7]{3,2,1,0}), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, feature_group_count=192, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
            "2025-12-10 14:42:51.880726: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 14:42:52.106931: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "I0000 00:00:1765377786.667972   67875 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732ms/step - accuracy: 0.0844 - loss: 5.1209 - top5_acc: 0.1848"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-10 14:51:29.247457: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:30.442291: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:30.622576: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:30.728646: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:31.113894: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:31.192352: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:31.460153: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:31.877514: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 664 bytes spill stores, 664 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:32.056610: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:32.057002: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:34.021596: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:34.161406: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:34.279002: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 10956 bytes spill stores, 10116 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:34.384264: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 1616 bytes spill stores, 1212 bytes spill loads\n",
            "\n",
            "2025-12-10 14:51:34.417579: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 360 bytes spill stores, 360 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from None to 0.41906, saving model to model_stage1_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 885ms/step - accuracy: 0.1778 - loss: 4.3371 - top5_acc: 0.3496 - val_accuracy: 0.4191 - val_loss: 2.5530 - val_top5_acc: 0.7243 - learning_rate: 3.0000e-04\n",
            "Epoch 2/8\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739ms/step - accuracy: 0.3910 - loss: 2.6875 - top5_acc: 0.6808\n",
            "Epoch 2: val_accuracy improved from 0.41906 to 0.49916, saving model to model_stage1_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 865ms/step - accuracy: 0.4186 - loss: 2.4892 - top5_acc: 0.7112 - val_accuracy: 0.4992 - val_loss: 1.9513 - val_top5_acc: 0.8103 - learning_rate: 3.0000e-04\n",
            "Epoch 3/8\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825ms/step - accuracy: 0.4983 - loss: 1.9144 - top5_acc: 0.7924\n",
            "Epoch 3: val_accuracy improved from 0.49916 to 0.55312, saving model to model_stage1_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 924ms/step - accuracy: 0.5080 - loss: 1.8596 - top5_acc: 0.8008 - val_accuracy: 0.5531 - val_loss: 1.7192 - val_top5_acc: 0.8373 - learning_rate: 3.0000e-04\n",
            "Epoch 4/8\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821ms/step - accuracy: 0.5499 - loss: 1.5965 - top5_acc: 0.8248\n",
            "Epoch 4: val_accuracy improved from 0.55312 to 0.57504, saving model to model_stage1_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 916ms/step - accuracy: 0.5504 - loss: 1.5892 - top5_acc: 0.8289 - val_accuracy: 0.5750 - val_loss: 1.5992 - val_top5_acc: 0.8533 - learning_rate: 3.0000e-04\n",
            "Epoch 5/8\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679ms/step - accuracy: 0.5878 - loss: 1.4403 - top5_acc: 0.8578\n",
            "Epoch 5: val_accuracy did not improve from 0.57504\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 746ms/step - accuracy: 0.5870 - loss: 1.3994 - top5_acc: 0.8583 - val_accuracy: 0.5700 - val_loss: 1.5712 - val_top5_acc: 0.8642 - learning_rate: 3.0000e-04\n",
            "Epoch 6/8\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634ms/step - accuracy: 0.6074 - loss: 1.2524 - top5_acc: 0.8701\n",
            "Epoch 6: val_accuracy improved from 0.57504 to 0.59612, saving model to model_stage1_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 755ms/step - accuracy: 0.6139 - loss: 1.2496 - top5_acc: 0.8753 - val_accuracy: 0.5961 - val_loss: 1.4802 - val_top5_acc: 0.8693 - learning_rate: 3.0000e-04\n",
            "Epoch 7/8\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791ms/step - accuracy: 0.6409 - loss: 1.1172 - top5_acc: 0.8895\n",
            "Epoch 7: val_accuracy improved from 0.59612 to 0.60287, saving model to model_stage1_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 920ms/step - accuracy: 0.6344 - loss: 1.1424 - top5_acc: 0.8906 - val_accuracy: 0.6029 - val_loss: 1.4751 - val_top5_acc: 0.8744 - learning_rate: 3.0000e-04\n",
            "Epoch 8/8\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - accuracy: 0.6540 - loss: 1.0165 - top5_acc: 0.9090\n",
            "Epoch 8: val_accuracy improved from 0.60287 to 0.61804, saving model to model_stage1_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 731ms/step - accuracy: 0.6576 - loss: 1.0296 - top5_acc: 0.9059 - val_accuracy: 0.6180 - val_loss: 1.4250 - val_top5_acc: 0.8811 - learning_rate: 3.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Stage 1 COMPLETE! Model saved.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Stage 1: Train with frozen backbone\"\"\"\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 1: Training with FROZEN backbone\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(\n",
        "        learning_rate=3e-4,\n",
        "        weight_decay=1e-4\n",
        "    ),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
        ")\n",
        "\n",
        "# Train\n",
        "history1 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=8,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=get_callbacks('stage1', patience=4),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save\n",
        "model.save('model_after_stage1.keras')\n",
        "print(f\"\\nStage 1 COMPLETE! Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XtWPVPkYUz-P",
      "metadata": {
        "id": "XtWPVPkYUz-P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 2: Training with PARTIAL backbone unfreezing (top 30%)\n",
            "================================================================================\n",
            "\n",
            "Unfreezing top 30.0% of backbone layers\n",
            "Total backbone layers: 259\n",
            "Unfreezing from layer 181 onwards\n",
            "Trainable parameters after unfreezing: 25,566,666\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-10 16:06:40.922846: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f9414018830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2025-12-10 16:06:40.923192: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
            "2025-12-10 16:06:41.859234: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2025-12-10 16:06:46.699871: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91700\n",
            "2025-12-10 16:06:52.485092: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 16:06:53.231441: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 16:06:53.722830: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 16:06:54.130280: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 664 bytes spill stores, 664 bytes spill loads\n",
            "\n",
            "2025-12-10 16:06:54.211432: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 16:06:54.283629: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-12-10 16:06:56.607611: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 360 bytes spill stores, 360 bytes spill loads\n",
            "\n",
            "2025-12-10 16:06:59.976681: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:00.241175: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:00.332944: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:00.509223: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:00.731962: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:01.204652: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:01.514204: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:01.806452: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:01.826504: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng3{k11=0} for conv (f32[16,768,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,768,7,7]{3,2,1,0}, f32[768,1,7,7]{3,2,1,0}), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, feature_group_count=768, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
            "2025-12-10 16:07:02.052337: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:02.340228: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:02.679363: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:02.920344: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-12-10 16:07:02.932676: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.106242158s\n",
            "Trying algorithm eng3{k11=0} for conv (f32[16,768,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,768,7,7]{3,2,1,0}, f32[768,1,7,7]{3,2,1,0}), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, feature_group_count=768, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
            "I0000 00:00:1765382833.748303   80572 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782ms/step - accuracy: 0.6899 - loss: 0.9215 - top5_acc: 0.9245"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-10 16:16:02.236135: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:02.498233: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:03.692467: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:04.723217: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:05.260482: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:05.516152: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:06.135598: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:06.534070: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:06.880130: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:06.964133: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 664 bytes spill stores, 664 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:07.522165: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 10956 bytes spill stores, 10116 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:07.706880: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:07.727697: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:07.940975: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 360 bytes spill stores, 360 bytes spill loads\n",
            "\n",
            "2025-12-10 16:16:08.003896: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 1616 bytes spill stores, 1212 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from None to 0.62226, saving model to model_stage2_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 930ms/step - accuracy: 0.6812 - loss: 0.9455 - top5_acc: 0.9202 - val_accuracy: 0.6223 - val_loss: 1.3897 - val_top5_acc: 0.8853 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669ms/step - accuracy: 0.6956 - loss: 0.8747 - top5_acc: 0.9248\n",
            "Epoch 2: val_accuracy improved from 0.62226 to 0.62901, saving model to model_stage2_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 761ms/step - accuracy: 0.6897 - loss: 0.8754 - top5_acc: 0.9246 - val_accuracy: 0.6290 - val_loss: 1.3764 - val_top5_acc: 0.8879 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639ms/step - accuracy: 0.6921 - loss: 0.8560 - top5_acc: 0.9237\n",
            "Epoch 3: val_accuracy did not improve from 0.62901\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 706ms/step - accuracy: 0.6935 - loss: 0.8553 - top5_acc: 0.9277 - val_accuracy: 0.6256 - val_loss: 1.3708 - val_top5_acc: 0.8862 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692ms/step - accuracy: 0.7013 - loss: 0.8415 - top5_acc: 0.9268\n",
            "Epoch 4: val_accuracy improved from 0.62901 to 0.63575, saving model to model_stage2_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 779ms/step - accuracy: 0.7068 - loss: 0.8391 - top5_acc: 0.9257 - val_accuracy: 0.6358 - val_loss: 1.3597 - val_top5_acc: 0.8879 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - accuracy: 0.7186 - loss: 0.7645 - top5_acc: 0.9391\n",
            "Epoch 5: val_accuracy did not improve from 0.63575\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 802ms/step - accuracy: 0.7141 - loss: 0.7814 - top5_acc: 0.9361 - val_accuracy: 0.6358 - val_loss: 1.3579 - val_top5_acc: 0.8929 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677ms/step - accuracy: 0.7224 - loss: 0.7584 - top5_acc: 0.9379\n",
            "Epoch 6: val_accuracy improved from 0.63575 to 0.64250, saving model to model_stage2_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 747ms/step - accuracy: 0.7176 - loss: 0.7683 - top5_acc: 0.9343 - val_accuracy: 0.6425 - val_loss: 1.3392 - val_top5_acc: 0.8929 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - accuracy: 0.7277 - loss: 0.7288 - top5_acc: 0.9379\n",
            "Epoch 7: val_accuracy did not improve from 0.64250\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 436ms/step - accuracy: 0.7199 - loss: 0.7520 - top5_acc: 0.9364 - val_accuracy: 0.6358 - val_loss: 1.3768 - val_top5_acc: 0.8904 - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643ms/step - accuracy: 0.7341 - loss: 0.7177 - top5_acc: 0.9364\n",
            "Epoch 8: val_accuracy improved from 0.64250 to 0.64924, saving model to model_stage2_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 712ms/step - accuracy: 0.7299 - loss: 0.7289 - top5_acc: 0.9376 - val_accuracy: 0.6492 - val_loss: 1.3301 - val_top5_acc: 0.8954 - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754ms/step - accuracy: 0.7323 - loss: 0.7397 - top5_acc: 0.9436\n",
            "Epoch 9: val_accuracy improved from 0.64924 to 0.65261, saving model to model_stage2_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 835ms/step - accuracy: 0.7288 - loss: 0.7242 - top5_acc: 0.9436 - val_accuracy: 0.6526 - val_loss: 1.3246 - val_top5_acc: 0.8946 - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644ms/step - accuracy: 0.7487 - loss: 0.6625 - top5_acc: 0.9417\n",
            "Epoch 10: val_accuracy did not improve from 0.65261\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 704ms/step - accuracy: 0.7397 - loss: 0.6856 - top5_acc: 0.9392 - val_accuracy: 0.6459 - val_loss: 1.3220 - val_top5_acc: 0.8904 - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641ms/step - accuracy: 0.7449 - loss: 0.6799 - top5_acc: 0.9436\n",
            "Epoch 11: val_accuracy did not improve from 0.65261\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 702ms/step - accuracy: 0.7368 - loss: 0.6958 - top5_acc: 0.9433 - val_accuracy: 0.6383 - val_loss: 1.3139 - val_top5_acc: 0.8929 - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634ms/step - accuracy: 0.7588 - loss: 0.6492 - top5_acc: 0.9447\n",
            "Epoch 12: val_accuracy did not improve from 0.65261\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 704ms/step - accuracy: 0.7512 - loss: 0.6732 - top5_acc: 0.9452 - val_accuracy: 0.6492 - val_loss: 1.2979 - val_top5_acc: 0.8980 - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618ms/step - accuracy: 0.7374 - loss: 0.6764 - top5_acc: 0.9455\n",
            "Epoch 13: val_accuracy did not improve from 0.65261\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 688ms/step - accuracy: 0.7434 - loss: 0.6603 - top5_acc: 0.9483 - val_accuracy: 0.6509 - val_loss: 1.2930 - val_top5_acc: 0.8971 - learning_rate: 5.0000e-05\n",
            "Epoch 14/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653ms/step - accuracy: 0.7675 - loss: 0.6350 - top5_acc: 0.9545\n",
            "Epoch 14: val_accuracy improved from 0.65261 to 0.65430, saving model to model_stage2_best.keras\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 719ms/step - accuracy: 0.7623 - loss: 0.6304 - top5_acc: 0.9490 - val_accuracy: 0.6543 - val_loss: 1.2929 - val_top5_acc: 0.8980 - learning_rate: 5.0000e-05\n",
            "Epoch 15/15\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631ms/step - accuracy: 0.7572 - loss: 0.6266 - top5_acc: 0.9512\n",
            "Epoch 15: val_accuracy did not improve from 0.65430\n",
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 693ms/step - accuracy: 0.7541 - loss: 0.6331 - top5_acc: 0.9492 - val_accuracy: 0.6484 - val_loss: 1.2915 - val_top5_acc: 0.8980 - learning_rate: 5.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 14.\n",
            "\n",
            "Stage 2 COMPLETE! Model saved.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Stage 2: Unfreeze top 30% of backbone\"\"\"\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 2: Training with PARTIAL backbone unfreezing (top 30%)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model = tf.keras.models.load_model('model_after_stage1.keras') #balazs: switched it with next line\n",
        "\n",
        "# Unfreeze top layers\n",
        "unfreeze_top_layers(model, percentage=0.3)\n",
        "\n",
        "\n",
        "# Compile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.AdamW(\n",
        "        learning_rate=1e-4,# balazs: it was too low didnt learn. was 1e-5\n",
        "        weight_decay=1e-4\n",
        "    ),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
        ")\n",
        "\n",
        "# Train\n",
        "history2 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=get_callbacks('stage2', patience=5),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save\n",
        "model.save('model_after_stage2.keras')\n",
        "print(f\"\\nStage 2 COMPLETE! Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "KsXwRQdXU0Lh",
      "metadata": {
        "id": "KsXwRQdXU0Lh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 3: Training with FULL backbone unfreezing\n",
            "================================================================================\n",
            "Found 9488 validated image filenames belonging to 202 classes.\n",
            "Found 1186 validated image filenames belonging to 202 classes.\n",
            "Trainable parameters: 48,397,194\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-10 23:44:16.310624: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f20e0006e90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2025-12-10 23:44:16.310723: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
            "2025-12-10 23:44:17.574275: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2025-12-10 23:44:31.175575: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91700\n",
            "2025-12-10 23:44:41.110240: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:41.616222: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:42.151255: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_78', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:42.694502: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:42.763021: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 1616 bytes spill stores, 1212 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:42.838952: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:43.077718: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 10956 bytes spill stores, 10116 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:43.179012: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 664 bytes spill stores, 664 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:43.325775: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_216', 152 bytes spill stores, 152 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:43.419310: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:43.737772: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_217', 100 bytes spill stores, 100 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:44.678456: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:45.401819: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:45.634382: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:46.122675: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:46.992996: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 360 bytes spill stores, 360 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:48.234397: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_132', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:48.320019: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79', 1664 bytes spill stores, 1344 bytes spill loads\n",
            "\n",
            "2025-12-10 23:44:49.001499: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 11912 bytes spill stores, 10644 bytes spill loads\n",
            "\n",
            "I0000 00:00:1765410334.684216   18059 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8447 - loss: 0.4167 - top5_acc: 0.9690"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-10 23:53:25.628381: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:26.470727: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:26.546802: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:26.555769: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:26.765302: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:27.242867: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:27.625038: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:27.884118: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:27.914705: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:27.928751: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:28.055850: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:28.389107: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 1616 bytes spill stores, 1212 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:28.745486: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 10956 bytes spill stores, 10116 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:28.746716: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 664 bytes spill stores, 664 bytes spill loads\n",
            "\n",
            "2025-12-10 23:53:28.834121: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 360 bytes spill stores, 360 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from None to 0.74789, saving model to model_stage3_best.keras\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 207ms/step - accuracy: 0.8491 - loss: 0.4029 - top5_acc: 0.9727 - val_accuracy: 0.7479 - val_loss: 1.0769 - val_top5_acc: 0.9216 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8617 - loss: 0.3690 - top5_acc: 0.9764\n",
            "Epoch 2: val_accuracy improved from 0.74789 to 0.74874, saving model to model_stage3_best.keras\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 199ms/step - accuracy: 0.8553 - loss: 0.3765 - top5_acc: 0.9742 - val_accuracy: 0.7487 - val_loss: 1.0657 - val_top5_acc: 0.9250 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8653 - loss: 0.3561 - top5_acc: 0.9758\n",
            "Epoch 3: val_accuracy improved from 0.74874 to 0.75211, saving model to model_stage3_best.keras\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 199ms/step - accuracy: 0.8641 - loss: 0.3507 - top5_acc: 0.9763 - val_accuracy: 0.7521 - val_loss: 1.0472 - val_top5_acc: 0.9258 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8687 - loss: 0.3288 - top5_acc: 0.9776\n",
            "Epoch 4: val_accuracy improved from 0.75211 to 0.76054, saving model to model_stage3_best.keras\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 198ms/step - accuracy: 0.8744 - loss: 0.3227 - top5_acc: 0.9799 - val_accuracy: 0.7605 - val_loss: 1.0391 - val_top5_acc: 0.9258 - learning_rate: 1.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8809 - loss: 0.3002 - top5_acc: 0.9784\n",
            "Epoch 5: val_accuracy did not improve from 0.76054\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 194ms/step - accuracy: 0.8776 - loss: 0.3027 - top5_acc: 0.9796 - val_accuracy: 0.7555 - val_loss: 1.0494 - val_top5_acc: 0.9300 - learning_rate: 1.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8836 - loss: 0.2761 - top5_acc: 0.9826\n",
            "Epoch 6: val_accuracy improved from 0.76054 to 0.76476, saving model to model_stage3_best.keras\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 198ms/step - accuracy: 0.8832 - loss: 0.2807 - top5_acc: 0.9820 - val_accuracy: 0.7648 - val_loss: 1.0324 - val_top5_acc: 0.9275 - learning_rate: 1.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8994 - loss: 0.2537 - top5_acc: 0.9875\n",
            "Epoch 7: val_accuracy improved from 0.76476 to 0.77066, saving model to model_stage3_best.keras\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 197ms/step - accuracy: 0.8968 - loss: 0.2540 - top5_acc: 0.9852 - val_accuracy: 0.7707 - val_loss: 1.0150 - val_top5_acc: 0.9283 - learning_rate: 1.0000e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8926 - loss: 0.2569 - top5_acc: 0.9873\n",
            "Epoch 8: val_accuracy improved from 0.77066 to 0.77403, saving model to model_stage3_best.keras\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 196ms/step - accuracy: 0.8988 - loss: 0.2467 - top5_acc: 0.9870 - val_accuracy: 0.7740 - val_loss: 1.0120 - val_top5_acc: 0.9275 - learning_rate: 1.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9070 - loss: 0.2209 - top5_acc: 0.9866\n",
            "Epoch 9: val_accuracy did not improve from 0.77403\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 193ms/step - accuracy: 0.9060 - loss: 0.2252 - top5_acc: 0.9860 - val_accuracy: 0.7740 - val_loss: 1.0114 - val_top5_acc: 0.9342 - learning_rate: 1.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9137 - loss: 0.2026 - top5_acc: 0.9882\n",
            "Epoch 10: val_accuracy did not improve from 0.77403\n",
            "\u001b[1m2372/2372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 195ms/step - accuracy: 0.9132 - loss: 0.2055 - top5_acc: 0.9876 - val_accuracy: 0.7732 - val_loss: 1.0032 - val_top5_acc: 0.9325 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Stage 3 COMPLETE! Model saved.\n"
          ]
        }
      ],
      "source": [
        "# Stage 3: Fine-tune all layers0\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 3: Training with FULL backbone unfreezing\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model = tf.keras.models.load_model('model_after_stage2.keras')\n",
        "\n",
        "# balazs: added lot of things here because it was keep crashing so i had to adjust it for my gpu\n",
        "\n",
        "# --- Mixed precision ---\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "# --- Recreate generators with smaller batch size if needed ---\n",
        "NEW_BATCH_SIZE = 4\n",
        "\n",
        "train_ds_stage3 = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='full_path',\n",
        "    y_col='family',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=NEW_BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_ds_stage3 = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='full_path',\n",
        "    y_col='family',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=NEW_BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\"\"\"\n",
        "# Unfreeze all layers\n",
        "base_model = model.layers[1]\n",
        "base_model.trainable = True\"\"\"\n",
        "\n",
        "# Unfreeze all layers but freeze BatchNorm layers\n",
        "base_model = model.layers[1]\n",
        "for layer in base_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "\n",
        "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Compile with even lower learning rate\n",
        "opt = tf.keras.optimizers.AdamW(\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=1e-5\n",
        ")\n",
        "opt = mixed_precision.LossScaleOptimizer(opt)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
        ")\n",
        "\n",
        "# Train\n",
        "history3 = model.fit(\n",
        "    train_ds_stage3,\n",
        "    validation_data=val_ds_stage3,\n",
        "    epochs=20,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=get_callbacks('stage3', patience=5),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save\n",
        "model.save('model_after_stage3.keras')\n",
        "print(f\"\\nStage 3 COMPLETE! Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "04ae8cc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def error_analysis(model, test_ds_eval, class_indices, normalize=True, top_k=15, save_prefix=\"test\"):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ERROR ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    test_ds_eval.reset()\n",
        "    y_pred_proba = model.predict(test_ds_eval, verbose=1)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "    y_true = test_ds_eval.classes\n",
        "\n",
        "    idx_to_class = {v: k for k, v in class_indices.items()}\n",
        "    class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
        "\n",
        "    if normalize:\n",
        "        cm_plot = cm.astype(\"float\") / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
        "        title = \"Confusion Matrix (row-normalized)\"\n",
        "    else:\n",
        "        cm_plot = cm\n",
        "        title = \"Confusion Matrix (counts)\"\n",
        "\n",
        "    # plot (sok classnál ne rakj tick labelt)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.imshow(cm_plot)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.colorbar()\n",
        "    plt.xticks([]); plt.yticks([])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_prefix}_confusion_matrix.png\", dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "    # report\n",
        "    report = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=class_names,\n",
        "        digits=4,\n",
        "        zero_division=0\n",
        "    )\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(report)\n",
        "\n",
        "    # top confused pairs\n",
        "    cm_off = cm.copy()\n",
        "    np.fill_diagonal(cm_off, 0)\n",
        "    pairs = []\n",
        "    for i in range(cm_off.shape[0]):\n",
        "        for j in range(cm_off.shape[1]):\n",
        "            if cm_off[i, j] > 0:\n",
        "                pairs.append((cm_off[i, j], class_names[i], class_names[j]))\n",
        "    pairs.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    print(f\"\\nTop {top_k} confusions (True -> Pred, count):\")\n",
        "    for cnt, t, p in pairs[:top_k]:\n",
        "        print(f\"  {t} -> {p}: {cnt}\")\n",
        "\n",
        "    return cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c28882",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1186 validated image filenames belonging to 202 classes.\n",
            "\n",
            "================================================================================\n",
            "ERROR ANALYSIS\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-14 22:35:50.823192: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f03000042d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2025-12-14 22:35:50.823281: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
            "2025-12-14 22:35:51.023566: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2025-12-14 22:35:51.773921: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91700\n",
            "2025-12-14 22:35:54.815235: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:55.020507: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:55.067452: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:55.079517: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:55.636716: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:55.746564: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:57.250844: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:57.345594: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 1616 bytes spill stores, 1212 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:57.461803: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:57.525274: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12', 664 bytes spill stores, 664 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:57.720620: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 10956 bytes spill stores, 10116 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:57.877837: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:58.218265: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:58.490706: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-12-14 22:35:58.933327: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 360 bytes spill stores, 360 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m  2/593\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 81ms/step   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1765751765.705735   11912 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m175/593\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 53ms/step"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/tf_gpu_test/.venv/lib/python3.12/site-packages/PIL/Image.py:3432: DecompressionBombWarning: Image size (115600000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 52ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x1000 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     accipitridae     0.7500    0.7500    0.7500        12\n",
            "    acipenseridae     0.8750    0.7778    0.8235         9\n",
            "      acroporidae     0.8500    0.8500    0.8500        20\n",
            "         agamidae     0.6667    0.6667    0.6667         6\n",
            "      agariciidae     1.0000    0.6667    0.8000        12\n",
            "        albulidae     0.6667    0.6667    0.6667         3\n",
            "      alcedinidae     0.6667    0.6667    0.6667         3\n",
            "    alligatoridae     0.6667    0.6667    0.6667         3\n",
            "        alopiidae     1.0000    0.8333    0.9091         6\n",
            "   ambystomatidae     0.3333    0.3333    0.3333         6\n",
            "         anatidae     0.8824    0.8824    0.8824        17\n",
            "         anguidae     0.7500    1.0000    0.8571         6\n",
            "          aotidae     0.8000    0.6667    0.7273         6\n",
            "           apidae     1.0000    0.9333    0.9655        15\n",
            "         ardeidae     0.7500    1.0000    0.8571         6\n",
            "   arthroleptidae     1.0000    1.0000    1.0000         3\n",
            "         atelidae     0.6875    0.7333    0.7097        15\n",
            "      attelabidae     1.0000    0.6667    0.8000         3\n",
            " balaenicipitidae     0.6000    1.0000    0.7500         3\n",
            "       balaenidae     1.0000    0.6667    0.8000         3\n",
            "  balaenopteridae     0.7500    1.0000    0.8571         3\n",
            "       balistidae     0.6667    0.3333    0.4444         6\n",
            "    bombycillidae     0.6667    0.6667    0.6667         3\n",
            "          bovidae     0.9500    0.8261    0.8837        23\n",
            "brachypteraciidae     1.0000    1.0000    1.0000         3\n",
            "      bucerotidae     0.8421    0.8000    0.8205        20\n",
            "        bufonidae     0.9000    0.6429    0.7500        14\n",
            "       burhinidae     1.0000    0.6667    0.8000         6\n",
            "       cacatuidae     1.0000    1.0000    1.0000         3\n",
            "   callitrichidae     0.5833    0.5833    0.5833        12\n",
            "  callorhinchidae     0.4000    0.6667    0.5000         3\n",
            "    caprimulgidae     0.7500    1.0000    0.8571         3\n",
            "        carabidae     1.0000    0.6667    0.8000         6\n",
            "   carcharhinidae     0.9524    0.7692    0.8511        26\n",
            "        cardiidae     0.8333    0.8333    0.8333         6\n",
            " carettochelyidae     0.5000    0.3333    0.4000         3\n",
            "          cebidae     0.7778    0.7778    0.7778         9\n",
            "     cerambycidae     0.6667    0.6667    0.6667         3\n",
            "  cercopithecidae     0.8387    0.8966    0.8667        29\n",
            "         cervidae     0.5000    0.6667    0.5714         3\n",
            "     cetorhinidae     1.0000    0.6667    0.8000         3\n",
            "   chaetodontidae     0.8571    1.0000    0.9231         6\n",
            "   chamaeleonidae     1.0000    0.6667    0.8000         6\n",
            "     charadriidae     0.8571    1.0000    0.9231         6\n",
            "   cheirogaleidae     1.0000    1.0000    1.0000         3\n",
            "         chelidae     1.0000    0.6667    0.8000         6\n",
            "      cheloniidae     0.5000    1.0000    0.6667         3\n",
            "      chelydridae     0.6667    0.6667    0.6667         3\n",
            "       ciconiidae     0.8182    1.0000    0.9000         9\n",
            "   coenagrionidae     1.0000    1.0000    1.0000         3\n",
            "       colubridae     0.6429    1.0000    0.7826         9\n",
            "       columbidae     1.0000    1.0000    1.0000         3\n",
            "          conidae     1.0000    1.0000    1.0000         3\n",
            "         cracidae     0.7500    1.0000    0.8571         6\n",
            "       cricetidae     1.0000    1.0000    1.0000         3\n",
            "     crocodylidae     0.6250    0.8333    0.7143         6\n",
            " cryptobranchidae     0.3750    0.5000    0.4286         6\n",
            "      ctenomyidae     1.0000    1.0000    1.0000         3\n",
            "        cuculidae     1.0000    1.0000    1.0000         3\n",
            "  cyprinodontidae     0.2500    0.3333    0.2857         3\n",
            "      dactyloidae     0.8846    0.7931    0.8364        29\n",
            "       dalatiidae     1.0000    1.0000    1.0000         3\n",
            "       dasyatidae     0.9231    0.8000    0.8571        15\n",
            "      dasypodidae     0.7500    1.0000    0.8571         3\n",
            "       dasyuridae     0.7500    1.0000    0.8571         3\n",
            "   daubentoniidae     0.7500    1.0000    0.8571         3\n",
            "      delphinidae     0.7500    0.5455    0.6316        11\n",
            "    dendrobatidae     0.7500    1.0000    0.8571         3\n",
            " dendrophylliidae     0.3750    1.0000    0.5455         3\n",
            "      diomedeidae     1.0000    0.9412    0.9697        17\n",
            "  diploastraeidae     0.7500    1.0000    0.8571         3\n",
            "  diplodactylidae     1.0000    0.3333    0.5000         6\n",
            "         elapidae     1.0000    0.6667    0.8000         3\n",
            "         emydidae     0.5000    0.3333    0.4000         6\n",
            "          equidae     1.0000    0.6667    0.8000         3\n",
            "      estrildidae     1.0000    1.0000    1.0000         3\n",
            "     euphylliidae     0.6667    0.8889    0.7619         9\n",
            "       falconidae     1.0000    0.5000    0.6667         6\n",
            "         faviidae     0.8333    0.8333    0.8333         6\n",
            "       formicidae     0.9655    0.9655    0.9655        29\n",
            "     fringillidae     1.0000    0.8333    0.9091         6\n",
            "        fungiidae     1.0000    1.0000    1.0000         3\n",
            "       gavialidae     0.0000    0.0000    0.0000         3\n",
            "       gekkonidae     0.7500    1.0000    0.8571         3\n",
            "      geoemydidae     0.6000    0.3333    0.4286         9\n",
            "       giraffidae     1.0000    1.0000    1.0000         3\n",
            "      glareolidae     0.7500    1.0000    0.8571         3\n",
            "         gliridae     0.7500    1.0000    0.8571         3\n",
            "        gomphidae     1.0000    1.0000    1.0000         6\n",
            "        goodeidae     0.6000    1.0000    0.7500         3\n",
            "       gymnuridae     0.3333    0.3333    0.3333         3\n",
            "       haliotidae     0.6000    1.0000    0.7500         3\n",
            "     helioporidae     0.0000    0.0000    0.0000         3\n",
            "   hemiscylliidae     1.0000    0.6667    0.8000         3\n",
            "      hexanchidae     0.7500    1.0000    0.8571         6\n",
            "        hominidae     1.0000    0.6667    0.8000         3\n",
            "        hyaenidae     0.5000    0.6667    0.5714         3\n",
            "      hylobatidae     0.6000    0.6667    0.6316         9\n",
            "       hynobiidae     0.1250    0.3333    0.1818         3\n",
            "        iguanidae     0.9231    1.0000    0.9600        12\n",
            "        indriidae     1.0000    1.0000    1.0000         3\n",
            "         labridae     0.5000    0.6667    0.5714         6\n",
            "       lacertidae     0.7500    1.0000    0.8571         6\n",
            "         lamnidae     0.5000    0.6667    0.5714         3\n",
            "          laridae     0.7857    0.7333    0.7586        15\n",
            "     latimeriidae     0.5000    0.6667    0.5714         3\n",
            "        lemuridae     0.6667    0.6667    0.6667         6\n",
            "        leporidae     0.7500    1.0000    0.8571         6\n",
            "   lobophylliidae     1.0000    1.0000    1.0000         3\n",
            "        lucanidae     0.7500    1.0000    0.8571         3\n",
            "       lutjanidae     0.5000    0.6667    0.5714         3\n",
            "          manidae     1.0000    0.3333    0.5000         6\n",
            "      mantellidae     0.6667    0.6667    0.6667         3\n",
            "     meandrinidae     0.5000    0.3333    0.4000         3\n",
            "     megapodiidae     1.0000    0.6667    0.8000         3\n",
            "     merlucciidae     0.7500    1.0000    0.8571         3\n",
            "      merulinidae     1.0000    0.5000    0.6667         6\n",
            "  mesitornithidae     1.0000    0.6667    0.8000         3\n",
            "          mimidae     1.0000    0.6667    0.8000         3\n",
            "     motacillidae     1.0000    1.0000    1.0000         3\n",
            "     muscicapidae     1.0000    1.0000    1.0000         3\n",
            "       mustelidae     1.0000    1.0000    1.0000         8\n",
            "     myliobatidae     0.7778    0.7778    0.7778         9\n",
            "    nesospingidae     0.6000    1.0000    0.7500         3\n",
            "      nymphalidae     1.0000    1.0000    1.0000         3\n",
            "   odontophoridae     1.0000    1.0000    1.0000         3\n",
            "        otariidae     0.7143    0.8333    0.7692         6\n",
            "         otididae     0.6000    1.0000    0.7500         3\n",
            "      palinuridae     1.0000    1.0000    1.0000         3\n",
            "      pangasiidae     0.6000    1.0000    0.7500         3\n",
            "     papilionidae     1.0000    1.0000    1.0000         3\n",
            "    paradisaeidae     1.0000    0.6667    0.8000         3\n",
            "     pardalotidae     0.6000    1.0000    0.7500         3\n",
            "        parulidae     0.6667    0.6667    0.6667         3\n",
            "         percidae     0.5000    0.6667    0.5714         3\n",
            "      phasianidae     1.0000    1.0000    1.0000         3\n",
            "  phrynosomatidae     0.3333    0.6667    0.4444         3\n",
            "  phyllomedusidae     1.0000    1.0000    1.0000         3\n",
            "   phyllostomidae     0.7500    1.0000    0.8571         3\n",
            "       pisauridae     1.0000    0.6667    0.8000         3\n",
            "         pittidae     1.0000    1.0000    1.0000         3\n",
            "   platystictidae     1.0000    1.0000    1.0000         3\n",
            "   plethodontidae     0.8125    0.4815    0.6047        27\n",
            "   pleuronectidae     1.0000    1.0000    1.0000         3\n",
            "   pocilloporidae     1.0000    1.0000    1.0000         6\n",
            "   podocnemididae     0.5000    1.0000    0.6667         3\n",
            "    polyprionidae     1.0000    0.6667    0.8000         3\n",
            "    pontoporiidae     0.5000    0.3333    0.4000         3\n",
            "       potoroidae     1.0000    0.6667    0.8000         3\n",
            "        pristidae     0.3333    0.3333    0.3333         3\n",
            "   procellariidae     0.8750    0.7778    0.8235         9\n",
            "pseudophasmatidae     0.7500    1.0000    0.8571         3\n",
            "      psittacidae     0.7857    0.9167    0.8462        12\n",
            "    psittaculidae     1.0000    0.8333    0.9091         6\n",
            "       pythonidae     0.8333    0.8333    0.8333         6\n",
            "          rajidae     0.6000    0.6667    0.6316         9\n",
            "         rallidae     1.0000    1.0000    1.0000         3\n",
            "     ramphastidae     1.0000    0.6667    0.8000         3\n",
            "          ranidae     0.6667    0.6667    0.6667         9\n",
            " recurvirostridae     0.7500    1.0000    0.8571         3\n",
            "    rhacophoridae     1.0000    0.8333    0.9091         6\n",
            "  rhinodermatidae     1.0000    1.0000    1.0000         3\n",
            " rhyacotritonidae     0.1818    0.3333    0.2353         6\n",
            "    salamandridae     0.6000    0.4615    0.5217        26\n",
            "       salmonidae     0.4000    0.6667    0.5000         3\n",
            "        scincidae     0.5000    0.3333    0.4000         3\n",
            "        sciuridae     1.0000    1.0000    1.0000         3\n",
            "     scolopacidae     1.0000    0.6667    0.8000         6\n",
            "       scombridae     0.4286    1.0000    0.6000         3\n",
            "       serranidae     1.0000    1.0000    1.0000         9\n",
            "   siderastreidae     0.6667    0.6667    0.6667         3\n",
            "        siluridae     0.6667    0.6667    0.6667         3\n",
            "      somniosidae     0.0000    0.0000    0.0000         3\n",
            "        soricidae     1.0000    0.3333    0.5000         3\n",
            "         sparidae     0.8000    0.6667    0.7273         6\n",
            "     spheniscidae     0.9091    0.9091    0.9091        11\n",
            "       sphyrnidae     0.5000    0.6667    0.5714         9\n",
            "        squalidae     0.6000    1.0000    0.7500         3\n",
            "      squatinidae     1.0000    0.6667    0.8000         3\n",
            "    stichopodidae     1.0000    1.0000    1.0000         6\n",
            "        strigidae     1.0000    0.6667    0.8000         3\n",
            "      strigopidae     0.6667    0.6667    0.6667         3\n",
            "     syngnathidae     0.6250    0.8333    0.7143         6\n",
            "     testudinidae     1.0000    1.0000    1.0000         6\n",
            "    tettigoniidae     0.7500    1.0000    0.8571         3\n",
            "    theraphosidae     1.0000    1.0000    1.0000         3\n",
            "       thraupidae     1.0000    0.8889    0.9412         9\n",
            "     trionychidae     0.5000    0.6667    0.5714         3\n",
            "       triopsidae     0.7500    1.0000    0.8571         3\n",
            "      trochilidae     1.0000    0.8889    0.9412         9\n",
            "       trogonidae     1.0000    1.0000    1.0000         3\n",
            "     tropiduridae     0.5000    1.0000    0.6667         3\n",
            "         turdidae     0.6667    0.6667    0.6667         6\n",
            "        unionidae     0.8333    0.8333    0.8333         6\n",
            "      urolophidae     0.6667    0.6667    0.6667         3\n",
            "          ursidae     0.8000    0.6667    0.7273         6\n",
            "         vangidae     0.6000    1.0000    0.7500         3\n",
            " vespertilionidae     1.0000    0.6667    0.8000         6\n",
            "        viperidae     0.7500    1.0000    0.8571         3\n",
            "       vireonidae     1.0000    0.3333    0.5000         3\n",
            "       vombatidae     1.0000    1.0000    1.0000         3\n",
            "        zonitidae     1.0000    0.6667    0.8000         3\n",
            "\n",
            "         accuracy                         0.7791      1186\n",
            "        macro avg     0.7838    0.7871    0.7664      1186\n",
            "     weighted avg     0.8072    0.7791    0.7781      1186\n",
            "\n",
            "\n",
            "Top 15 confusions (True -> Pred, count):\n",
            "  plethodontidae -> rhyacotritonidae: 6\n",
            "  plethodontidae -> salamandridae: 5\n",
            "  carcharhinidae -> sphyrnidae: 3\n",
            "  rhyacotritonidae -> hynobiidae: 3\n",
            "  salamandridae -> rhyacotritonidae: 3\n",
            "  acroporidae -> euphylliidae: 2\n",
            "  agariciidae -> dendrophylliidae: 2\n",
            "  balistidae -> labridae: 2\n",
            "  bovidae -> cervidae: 2\n",
            "  chamaeleonidae -> agamidae: 2\n",
            "  cryptobranchidae -> ambystomatidae: 2\n",
            "  dasyatidae -> gymnuridae: 2\n",
            "  delphinidae -> sphyrnidae: 2\n",
            "  diplodactylidae -> lacertidae: 2\n",
            "  geoemydidae -> podocnemididae: 2\n"
          ]
        }
      ],
      "source": [
        "# --- Load best model ONCE ---\n",
        "model = tf.keras.models.load_model('model_stage3_best.keras')\n",
        "\n",
        "# --- SMALL-BATCH EVAL GENERATOR ---\n",
        "EVAL_BS = 2\n",
        "test_ds_eval = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='full_path',\n",
        "    y_col='family',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=EVAL_BS,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Error analysis\n",
        "cm = error_analysis(\n",
        "    model,\n",
        "    test_ds_eval,\n",
        "    class_indices=test_ds_eval.class_indices,\n",
        "    normalize=True,\n",
        "    top_k=15,\n",
        "    save_prefix=\"test\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
