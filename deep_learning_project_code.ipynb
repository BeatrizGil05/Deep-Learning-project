{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv('metadata.csv')\n",
    "\n",
    "# Add the root directory with the unzipped images.\n",
    "data_root_path = r'C:\\Users\\inesb\\Downloads\\rare_species' \n",
    "df['full_path'] = df['file_path'].apply(lambda x: os.path.join(data_root_path, x))\n",
    "\n",
    "# Remove rows with missing file paths or family labels\n",
    "df = df.dropna(subset=['file_path', 'family']).reset_index(drop=True)\n",
    "\n",
    "# Check for missing files\n",
    "df['exists'] = df['full_path'].apply(os.path.exists)\n",
    "missing = df[df['exists'] == False]\n",
    "\n",
    "print(\"Missing images:\", len(missing))\n",
    "if len(missing) > 0:\n",
    "    print(missing[['file_path']].head())\n",
    "\n",
    "# Drop rows with missing images\n",
    "df = df[df['exists'] == True].reset_index(drop=True)\n",
    "\n",
    "# Duplicate rows in metadata\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(\"Duplicate metadata rows:\")\n",
    "print(duplicate_rows)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Duplicate image paths\n",
    "duplicate_paths = df[df.duplicated(subset='full_path')]\n",
    "print(\"Duplicate file paths:\")\n",
    "print(duplicate_paths)\n",
    "df = df.drop_duplicates(subset='full_path').reset_index(drop=True)\n",
    "\n",
    "# Encode each category in the target variable\n",
    "df['family_encoded'] = pd.factorize(df['family'])[0]\n",
    "unique_families = df['family'].unique()\n",
    "print(df['family'].nunique()) # 202\n",
    "df.head(3)\n",
    "\n",
    "# Check the class distribution throughout the dataset\n",
    "target_distribution = df['full_path'].groupby(df[\"family_encoded\"]).count()\n",
    "print(target_distribution.describe().T)\n",
    "# The count of images per class varies from 29 to 300, so we can consider this an imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Split: 70% Train, 15% Validation, 15% Test\n",
    "train_df, test_df = train_test_split(df, test_size = 0.15, stratify = df['family'], random_state = 42)\n",
    "train_df, val_df = train_test_split(train_df, test_size = 0.1765, stratify = train_df['family'], random_state = 42) \n",
    "# (0.1765 of the remaining 85% is roughly 15% of the total)\n",
    "print(f\"Train shape: {train_df.shape}\") # Train shape: (8387, 10)\n",
    "print(f\"Val shape: {val_df.shape}\") # Val shape: (1798, 10)\n",
    "print(f\"Test shape: {test_df.shape}\") # Test shape: (1798, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
